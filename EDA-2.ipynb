{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <h1 align=\"center\">Plant Classification</h1>\n",
        "    <h3 align=\"center\">A2NDL POLIMI</h3>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "vo-FU0uSzSsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-Mount Google drive and unzip data"
      ],
      "metadata": {
        "id": "2Vyl23BS0Qly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # mount google drive to /content directory\n",
        "!unzip -qq \"/content/drive/MyDrive/public_data.zip\" -d \"/content\"  # unzip dataset to the /content directory"
      ],
      "metadata": {
        "id": "pXEekFrYzSKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3645e354-5489-47a0-95e4-8f898fd57d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "unzip:  cannot find or open /content/drive/MyDrive/public_data.zip, /content/drive/MyDrive/public_data.zip.zip or /content/drive/MyDrive/public_data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-Import Libraries"
      ],
      "metadata": {
        "id": "WdIAHH3y1MGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, datetime, warnings, logging, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras import callbacks\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.layers import Dropout\n",
        "import keras,cv2\n",
        "from keras import optimizers\n",
        "from sklearn import preprocessing\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2, glob, random, os, time, shutil, datetime,time, keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D, BatchNormalization, Activation, Input, Add, Dense, ZeroPadding2D,Flatten, AveragePooling2D, Rescaling\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import layers, callbacks , metrics\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "qH4lmQYsrspb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-Fix randomness and hide warnings"
      ],
      "metadata": {
        "id": "P2ULXjC2r2G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "#####\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "#####\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "#####\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "oEYVRRdgMuWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-Set first parameters of tensorflow"
      ],
      "metadata": {
        "id": "pua8lra2sVWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tensorflow\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(\"tensorflow_version\",tf.__version__)"
      ],
      "metadata": {
        "id": "WC9nG5HlMvHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7f1b90-3c4b-46bd-aef9-f82507d7f971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow_version 2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-Load dataset"
      ],
      "metadata": {
        "id": "-WhQFn5S1D3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.load(\"/content/public_data.npz\", allow_pickle=True)"
      ],
      "metadata": {
        "id": "7Sfce8o_08iF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "bdefda07-6013-4c57-8516-3394e40640e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ee0165634c34>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/public_data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/public_data.npz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-EDA On Dataset"
      ],
      "metadata": {
        "id": "Ub3zPKGT1fiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Watch keys available in the numpy array\n",
        "keys = list(dataset.keys())\n",
        "print('keys in our dataset are: ', keys)\n",
        "print('*'*100)\n",
        "\n",
        "# look at data shape\n",
        "x = dataset[\"data\"]\n",
        "no_images = x.shape[0]\n",
        "size_images = x.shape[1:3]\n",
        "print('Data shape: ',x.shape)\n",
        "print('*'*100)\n",
        "\n",
        "# Lool aat labels\n",
        "y = dataset[\"labels\"]\n",
        "no_labels = y.shape[0]\n",
        "print(\"Labels are : \",y)\n",
        "print('*'*100)\n",
        "\n",
        "# Look at bing balanced or imbalanced\n",
        "_, counts = np.unique(y,return_counts=True) # count occurrence of each item\n",
        "no_healthy_images = counts[0]\n",
        "no_unhealthy_images = counts[1]\n",
        "\n",
        "# pass variables to a dictionary to be used as dataframe for a better show\n",
        "info_table_dict = {\"no_images\":no_images, \"image_width\": size_images[0],\"image_length\": size_images[1], \"no_labels\":no_labels,\n",
        "                   \"no_healthy_images\":no_healthy_images,\"percentage%\":no_healthy_images*100/no_labels,\n",
        "                   \"no_unhealthy_images\":no_unhealthy_images,\"percentage %\":no_unhealthy_images*100/no_labels }\n",
        "info_table = pd.DataFrame(info_table_dict, index =['value'])\n",
        "info_table"
      ],
      "metadata": {
        "id": "Ay-MIDxM1dvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show histogram of y(labels) to investigate wheather dataset is balanced or not\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(y, bins=3,align='mid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Ev0bRMB1txc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn healty and unhealthy labels to two classes of 0 and 1, using labelEncoder\n",
        "labels_before_encoder = y\n",
        "\n",
        "# Create a LabelEncoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "# Fit and transform the encoder on the original array\n",
        "labels_after_encoder = label_encoder.fit_transform(y)\n",
        "\n",
        "# set y  to new values\n",
        "y = labels_after_encoder\n",
        "\n",
        "\n",
        "print(f\"before label encoding, list is like this :{labels_before_encoder}. And, unique items are: {np.unique(labels_before_encoder)}\")\n",
        "print(f\"after label encoding, list is like this :{labels_after_encoder}. And, unique items are: {np.unique(labels_after_encoder)}\")"
      ],
      "metadata": {
        "id": "eu4sX-LHAJji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we want to use softamx activation in last layer of each model\n",
        "# we must turn y to categorical labels\n",
        "y = tfk.utils.to_categorical(y,2)\n",
        "print(f\"after label encoding, list is like this :{labels_after_encoder}. And, unique items are: {np.unique(labels_after_encoder)}\")\n",
        "print(f\"after turning to categorical, list is like this :{y}. And, unique items are: {np.unique(y)}\")"
      ],
      "metadata": {
        "id": "0gtdzMNWHBM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7_Show images"
      ],
      "metadata": {
        "id": "KI7xuMEbZFPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# A function for displaying images in batches\n",
        "def show_images(dataset, labels, batch_no, no_images_per_batch, num_cols=10):\n",
        "    '''input: dataset wiht shape(None,width,lenght,channel),\n",
        "              labels with this format [[1 0]\n",
        "                                       [1 0]],\n",
        "              batch_no says which section/batch of your data you want to show,\n",
        "              no_images_per_batch says how many images you want to put into one batch\n",
        "       output: show images you want'''\n",
        "\n",
        "    start_index = batch_no * no_images_per_batch\n",
        "\n",
        "    end_index = min((batch_no + 1) * no_images_per_batch, len(dataset))\n",
        "\n",
        "    num_rows = (end_index - start_index + num_cols - 1) // num_cols\n",
        "    fig, axes = plt.subplots(num_rows, num_cols)\n",
        "    label = tf.argmax(labels, axis=-1).numpy()\n",
        "\n",
        "    for i, ax in enumerate(axes.ravel()):\n",
        "        if start_index + i < end_index:\n",
        "            image = dataset[start_index + i]\n",
        "            image = image / 255.0  # Normalize pixel values to [0, 1]\n",
        "            ax.imshow(image)\n",
        "            ax.set_title(f\"{label[start_index + i]}\",fontsize=10, y=-0.5)\n",
        "            ax.axis('off')\n",
        "\n",
        "    for i in range(end_index - start_index, num_rows * num_cols):\n",
        "        fig.delaxes(axes.flatten()[i])\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bf1SDKlIZbCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(dataset=x,labels =y, batch_no=0,no_images_per_batch=40)"
      ],
      "metadata": {
        "id": "jWSVSRfKZdOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8-Splitting data"
      ],
      "metadata": {
        "id": "PrB7sWt7urCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly, we split dataset to training_data and test part. We keep x_test, y_test till the end for evaluating the model\n",
        "x_training_data, x_test, y_training_data, y_test = train_test_split(\n",
        "    x,\n",
        "    y,\n",
        "    test_size = 0.1,\n",
        "    shuffle =True,\n",
        "    random_state=seed,\n",
        "    stratify=y)# I used stratified since our data is imbalanced\n"
      ],
      "metadata": {
        "id": "-ZS-_bkGPUVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Secondly, We split training data into train and validation parts. Training part\n",
        "# for training and validation part to validate model.\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_training_data,\n",
        "    y_training_data,\n",
        "    test_size = len(x_test), # Ensure validation set size matches test set size--> Why??\n",
        "    random_state=seed,\n",
        "    shuffle =True,\n",
        "    stratify=y_training_data)"
      ],
      "metadata": {
        "id": "MCJ7dXG9PWSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shapes of different datsets after splitting the main dataset into training, validation, testing parts\n",
        "print('X_train.shape:',x_train.shape,'y_train.shape:', y_train.shape)\n",
        "print('X_val.shape:',x_val.shape, 'y_val.shape:',y_val.shape)\n",
        "print('X_test.shape:',x_test.shape,'y_test.shape', y_test.shape)"
      ],
      "metadata": {
        "id": "GClHE3gsPzbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-Watch data distribution"
      ],
      "metadata": {
        "id": "HcRCxxErQG54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create three separate subplots to visualize the sorted target variable values for the training, validation, and test sets\n",
        "\n",
        "# Create a figure for the training labels subplot\n",
        "plt.figure(figsize=(20, 2))\n",
        "plt.scatter((np.arange(y.shape[0])), np.sort(tf.argmax(y, axis=-1), axis=0))\n",
        "plt.title('Training labels')\n",
        "plt.xticks([])  # Remove x-axis ticks\n",
        "plt.grid(0.2)   # Add a grid with opacity 0.2 for reference\n",
        "plt.ylim([-0.1, 1.1])  # Set the y-axis limits to [0.1, 1.1]\n",
        "\n",
        "# Create a figure for the validation labels subplot\n",
        "plt.figure(figsize=(20, 2))\n",
        "plt.scatter(np.arange(y_val.shape[0]), np.sort(tf.argmax(y_val, axis=-1), axis=0))\n",
        "plt.title('Validation labels')\n",
        "plt.xticks([])  # Remove x-axis ticks\n",
        "plt.grid(0.2)   # Add a grid with opacity 0.2 for reference\n",
        "plt.ylim([-0.1, 1.1])  # Set the y-axis limits to [0.1, 1.1]\n",
        "\n",
        "# Create a figure for the test labels subplot\n",
        "plt.figure(figsize=(20, 2))\n",
        "plt.scatter(np.arange(y_test.shape[0]), np.sort(tf.argmax(y_test, axis=-1), axis=0))\n",
        "plt.title('Test labels')\n",
        "plt.xticks([])  # Remove x-axis ticks\n",
        "plt.grid(0.2)   # Add a grid with opacity 0.2 for reference\n",
        "plt.ylim([-0.1, 1.1])  # Set the y-axis limits to [0.1, 1.1]\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jK_uMJirQGRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10- Feature Scaling"
      ],
      "metadata": {
        "id": "J7ITNoQQHSNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train/255.0\n",
        "x_val = x_val/255.0\n",
        "x_test = x_test/255.0"
      ],
      "metadata": {
        "id": "OfZdzyA2xnYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}